{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411b243e-47d2-4dc5-8a35-163548cb3915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from torch.distributions.categorical import Categorical\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45ba0e8-4e47-4e18-a344-0feff04b0d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = 'ppo'\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 12\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = True\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    track: bool = False\n",
    "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "\n",
    "    # Algorithm simulation arguments\n",
    "    env_id: str = \"CartPole-v1\"\n",
    "    \"\"\"the id of the environment\"\"\"\n",
    "    total_timesteps: int = 1000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    num_envs: int = 4\n",
    "    \"\"\"the number of parallel game environments\"\"\"\n",
    "    num_steps: int = 128\n",
    "    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n",
    "    gamma: float = 0.99\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "\n",
    "    #Frequency simulation arguments\n",
    "    M: int = 1000\n",
    "    \"\"\" the number of frequency samples is M\"\"\"\n",
    "    sigmaOmega: float = 1\n",
    "    \"\"\" the standard deviation of the normal distribution from which the frequency is sampled \"\"\"\n",
    "    \n",
    "    # traing arguments for critic\n",
    "    batch_size: int = 2048\n",
    "    \"\"\"batch_size for critic training\"\"\"\n",
    "    epochs: int = 4\n",
    "    \"\"\"number of epochs for critic training\"\"\"\n",
    "    learning_rate: float = 2.5e-4\n",
    "    \"\"\"learning for critic training\"\"\"\n",
    "    lw: float = 0.0\n",
    "    \"\"\"L2 regularization coefficient for critic training\"\"\"\n",
    "    \n",
    "    #to be filled at runtime\n",
    "    num_iterations: int = 0\n",
    "    \"\"\"the number of iterations (computed in runtime)\"\"\"\n",
    "    data_size_critic: int = 0\n",
    "    \"\"\"the number of dataset points over which critic is trained on (per iteration)\"\"\"\n",
    "    critic_state_size: int = 0\n",
    "    \"\"\"the number of states over which critic is trained on (per iteration)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf9248-316e-47f2-9aff-0b9ffd2dc6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "args.critic_state_size = int(args.num_envs * args.num_steps)\n",
    "args.num_iterations = args.total_timesteps // args.critic_state_size\n",
    "\n",
    "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "#writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "#writer.add_text(\n",
    "#        \"hyperparameters\",\n",
    "#        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "#    )\n",
    "print(args.num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb1030-fdf3-4ab0-b4ec-376e8c69bbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb43e99-f660-458c-942c-11c2fcdfa257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e5f772-a60a-445d-8620-7999dad97879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    \n",
    "    return layer\n",
    "\n",
    "class xsinx(nn.Module):\n",
    "    def __init__(self, a=1.0):\n",
    "        super(xsinx, self).__init__()\n",
    "        self.a = nn.Parameter(torch.tensor(a))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + (1.0 / self.a) * torch.sin(self.a * x) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad52ec98-54db-443a-abe3-4072cc870875",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n",
    "        )\n",
    "\n",
    "    def get_action(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ca7194-0988-4e88-9748-bb69efea17a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cf(torch.nn.Module):\n",
    "    def __init__(self, state_input_size, layer_sizes, sigma, sigma_opt=False):\n",
    "        super().__init__()\n",
    "        # Output size is twice the complex output size to account for real and imaginary parts\n",
    "        output_size = 2\n",
    "        self.layer_sizes = [state_input_size + 1] + layer_sizes + [output_size]\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(len(self.layer_sizes) - 2):\n",
    "            self.layers.append(layer_init(torch.nn.Linear(self.layer_sizes[i], self.layer_sizes[i + 1])))\n",
    "            self.layers.append(xsinx(a=0.5))\n",
    "        \n",
    "        self.layers.append(layer_init(torch.nn.Linear(self.layer_sizes[-2], self.layer_sizes[-1])))\n",
    "\n",
    "        if sigma_opt:\n",
    "            self.sigma = torch.nn.Parameter(sigma)\n",
    "        else:\n",
    "            self.sigma = sigma\n",
    "\n",
    "    def freq_sampling(self, num_samples):\n",
    "\n",
    "        std_sample = torch.distributions.Normal(0.0, 1.0).sample((num_samples,)).to(device)\n",
    "        return self.sigma * std_sample\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "\n",
    "        out = torch.zeros_like(x)\n",
    "        out[:, 0] = torch.cos(x[:, 0])\n",
    "        out[:, 1] = torch.sin(x[:, 1])        \n",
    "\n",
    "        # Split the output into real and imaginary parts and form the complex output\n",
    "        real = out[:,0]\n",
    "        imag = out[:,1]\n",
    "        return torch.complex(real, imag)\n",
    "\n",
    "    def get_mean(self,state):\n",
    "        omega_fixed = torch.tensor([0.0], requires_grad=True).expand(state.size(0)).unsqueeze(1).to(device)\n",
    "        eva = torch.cat((state,omega_fixed),dim=1)\n",
    "        output = self.forward(eva).imag\n",
    "        gradients = torch.autograd.grad(outputs=output, inputs=eva, grad_outputs=torch.ones_like(output),allow_unused=True)\n",
    "        return gradients[0][:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc04e01-f9ba-4374-b6ad-a70fb26dc6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_network = cf(4, [64, 64,64], 0.1,sigma_opt=False).to(device)\n",
    "model_optimizer = optim.Adam(cf_network.parameters(), weight_decay = args.lw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7c2164-54dd-41bc-9e02-6b0f8e15be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(args.env_id, i, args.capture_video, run_name) for i in range(args.num_envs)],\n",
    "    )\n",
    "agent = Agent(envs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a592e64-f473-4fea-ba1b-371855180bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "\n",
    "input_data = torch.zeros((args.num_steps, args.num_envs*args.M)+envs.single_observation_space.shape).to(device)\n",
    "output_data = torch.zeros((args.num_steps,args.num_envs*args.M),dtype=torch.complex128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88b819-814e-4003-be8e-8dc80164a489",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = envs.reset(seed=args.seed)\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(args.num_envs).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd49467a-ab86-44f1-8bb5-311a542a7e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_update(r, t, ns, model,ndone):\n",
    "    t_expanded = t.unsqueeze(0).expand(r.size(0), -1)\n",
    "    input_tensor = torch.cat((ns, args.gamma*t_expanded), dim=-1)\n",
    "    with torch.no_grad():\n",
    "        return (1.0-ndone)*torch.exp(1j*t*r).flatten() + ndone*model(input_tensor).flatten()*torch.exp(1j*t*r).flatten()\n",
    "\n",
    "# Function to apply td_update for a single t value\n",
    "def single_td(r, ns, t, model,ndone):\n",
    "    return td_update(r, t, ns, model,ndone)\n",
    "\n",
    "# Vectorize over t values using vmap\n",
    "def vectorized_td(r, ns, t_list, model,ndone):\n",
    "    return torch.vmap(lambda t: single_td(r, ns, t, model,ndone), in_dims=0, out_dims=0)(t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aafff67-cee7-4795-926b-f12eed25a2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_copies = []\n",
    "for iteration in range(1, args.num_iterations + 1):\n",
    "\n",
    "    std_sample = torch.randn(args.M).to(device)\n",
    "    omega = (args.sigmaOmega * std_sample).to(device)\n",
    "    #omega = torch.cat((omega, (torch.zeros(3*args.M)).to(device)), dim=0).to(device)\n",
    "    \n",
    "    for step in range(0, args.num_steps):\n",
    "        \n",
    "        global_step += args.num_envs\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action = agent.get_action(next_obs)\n",
    "        actions[step] = action\n",
    "    \n",
    "        next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "        next_done = np.logical_or(terminations, truncations)\n",
    "        rewards[step] = 0.25*torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n",
    "\n",
    "        if \"final_info\" in infos:\n",
    "            for info in infos[\"final_info\"]:\n",
    "                if info and \"episode\" in info:\n",
    "                    print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
    "                    #writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
    "                    #writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        returns = torch.zeros_like(rewards).to(device)\n",
    "        last = 0\n",
    "        for t in reversed(range(args.num_steps)):\n",
    "            if t == args.num_steps - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                output_data[t] = vectorized_td(rewards[t], next_obs, omega, cf_network,nextnonterminal).transpose(0,1).flatten()\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                output_data[t] = vectorized_td(rewards[t], obs[t+1], omega, cf_network,nextnonterminal).transpose(0,1).flatten()\n",
    "\n",
    "            delta = rewards[t]\n",
    "            returns[t] = last = delta + args.gamma*nextnonterminal * last\n",
    "            input_data[t] = obs[t].repeat_interleave(omega.shape[0], dim=0).to(device)\n",
    "            \n",
    "        \n",
    "        \n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    if iteration == 1:\n",
    "        t_obs = b_obs.clone()\n",
    "    else:\n",
    "        t_obs = torch.vstack((t_obs,b_obs))\n",
    "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_returns = returns.reshape(-1)\n",
    "\n",
    "    # Organize the input data with the collected frequency samples:\n",
    "\n",
    "    state_repeated = input_data.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    omega_repeated = omega.repeat(b_obs.shape[0]).to(device)\n",
    "    state_omega = torch.cat((state_repeated, omega_repeated.unsqueeze(1)), dim=1)\n",
    "\n",
    "    # Organize the target data with the collected frequency samples:\n",
    "\n",
    "    ecfSamples = output_data.reshape(-1)\n",
    "    args.data_size_critic = state_omega.shape[0]\n",
    "    num_batches = args.data_size_critic//args.batch_size \n",
    "\n",
    "    lossEpoch = np.zeros(args.epochs)\n",
    "    b_inds = np.arange(args.data_size_critic)\n",
    "\n",
    "    print(f'Size of critic training set is {len(b_inds)}')\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "    \n",
    "        frac = 1.0 - epoch/ args.epochs\n",
    "        lrnow = frac * args.learning_rate\n",
    "        model_optimizer.param_groups[0][\"lr\"] = lrnow\n",
    "\n",
    "        for i in range(num_batches):\n",
    "\n",
    "            start_idx = i * args.batch_size\n",
    "            end_idx = start_idx + args.batch_size\n",
    "\n",
    "            mb_inds = b_inds[start_idx:end_idx]\n",
    "            \n",
    "            inputs = state_omega[mb_inds]\n",
    "            targets = ecfSamples[mb_inds]\n",
    "\n",
    "            model_optimizer.zero_grad()  # Zero the gradients for the model parameters\n",
    "\n",
    "            outputs = cf_network(inputs)\n",
    "\n",
    "            loss_cfNN = torch.mean(torch.abs(outputs - targets)**2) \n",
    "            loss_cfNN.backward()\n",
    "            model_optimizer.step()\n",
    "    \n",
    "            lossEpoch[epoch]+=loss_cfNN.item()\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Loss: {lossEpoch[epoch]}')\n",
    "    \n",
    "    model_copies.append(copy.deepcopy(cf_network.state_dict()))\n",
    "    print(f'Iteration {iteration}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cedb9dd-bea4-40e4-8244-456817ebc7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_indices = torch.randperm(t_obs.size(0))\n",
    "t_obs = t_obs[permuted_indices]\n",
    "print(f'Size of observation set is {t_obs.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859f6802-0e70-4193-b9a4-ae42c94fdb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(omega, bins=20, edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea466aa-c14b-43a0-b26d-ec5da281ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_index = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f34fb9-24a2-4027-bbdf-fed69c36df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "st_state = t_obs[t_index,:].numpy()\n",
    "num_st = 100\n",
    "r = np.zeros(num_st)\n",
    "for i in range(num_st):\n",
    "    _,_ = env.reset()\n",
    "    episode_return = 0\n",
    "    env.state = st_state\n",
    "    \n",
    "    state = st_state\n",
    "    done = False\n",
    "    truncated = False\n",
    "    df = 1\n",
    "    while not done and not truncated:\n",
    "        action = agent.get_action(torch.Tensor(st_state)).numpy()\n",
    "        state, reward, done, truncated, _ = env.step(action)\n",
    "        episode_return += df*reward\n",
    "        df = df*args.gamma\n",
    "    r[i] = episode_return\n",
    "print(np.mean(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640c29c-d0d6-4c93-b210-fa9c2dcb1de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(r, bins=20, edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4d66e6-0ccf-4d35-818c-30cfa507dbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecf(samples,omega):\n",
    "    expo_samples = torch.exp(1j*omega*samples)  \n",
    "\n",
    "    return torch.mean(expo_samples, dim=0)\n",
    "\n",
    "vectorized_ecf = torch.vmap(ecf,in_dims=(None,0),chunk_size=100)\n",
    "freq_values = torch.linspace(-2, 2, 1000)\n",
    "ecfSamp = vectorized_ecf(torch.Tensor(r)*0.25, freq_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe5c09d-8749-4344-a63b-6955ad16d6f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cf_network.eval()\n",
    "state = (t_obs[t_index,:].repeat(1000,1))\n",
    "evalPlot = torch.cat((state,freq_values.unsqueeze(1)),dim=1)\n",
    "cfNN = cf_network(evalPlot).detach()\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "# plt.plot(freq_values.cpu(), ecf_eval.real.cpu(), label='Real Part (ECF)')\n",
    "plt.plot(freq_values, cfNN.real.cpu(), label='Real Part (NN)')\n",
    "plt.plot(freq_values, ecfSamp.real, label='Real Part (emp)')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Real Part')\n",
    "plt.title(f'Real Component')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(freq_values.cpu(), cfNN.imag.cpu(), label='Imaginary Part (NN)')\n",
    "plt.plot(freq_values.cpu(), ecfSamp.imag, label='Imaginary Part (emp)')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Imaginary Part')\n",
    "plt.title(f'Imaginary Component')\n",
    "plt.legend()\n",
    "plt.suptitle(f'Plot for state {state[0].numpy()} at time index {t_index}')\n",
    "plt.savefig(f't{t_index}'+f'_b{args.batch_size}'+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a861167a-f67d-4eb4-8b06-8dd8997af001",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({f'model_{i}': model for i, model in enumerate(model_copies)}, 'models_dict_batchs_'+f'{args.batch_size}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125bd25-8777-4ef8-9708-5fa8cd024b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = torch.load('models_dict_batchs_'+f'{args.batch_size}.pt')\n",
    "cf_network.load_state_dict(models_dict[f'model_{args.num_iterations-1}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afd515f-d32b-47fe-a065-0e491f9066ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
